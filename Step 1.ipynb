{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d14185e",
   "metadata": {},
   "source": [
    "\n",
    "## Step 1: Selecting a Category and Scraping Company Names and Links\n",
    "\n",
    "The initial phase of our scraping project involves selecting a specific category from the NSW supplier portal. For illustrative purposes, let's consider we have chosen the category \"Agriculture.\"\n",
    "\n",
    "Once the category is selected, the next task is to run a script that is designed to perform two primary functions:\n",
    "\n",
    "1. **Scrape Company Names:**\n",
    "   - The script will navigate through the \"Agriculture\" category on the NSW supplier portal.\n",
    "   - It will methodically identify and extract the names of companies listed under this category.\n",
    "\n",
    "2. **Extract Links to NSW Portal of Companies:**\n",
    "   - Concurrently, the script will also capture the unique URL link associated with each company's specific page on the NSW portal.\n",
    "   - These links are crucial as they lead to more detailed information about each company, which will be utilized in the subsequent steps of the scraping process.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99198e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get update\n",
    "!apt install chromium-chromedriver\n",
    "!pip install selenium\n",
    "!pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fdaa368",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import time\n",
    "\n",
    "# Setup Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument('--headless')\n",
    "chrome_options.add_argument('--no-sandbox')\n",
    "chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def get_supplier_data(page):\n",
    "    url = f\"https********ADD THE URL HERE *********&page={page}\" #Add url\n",
    "    driver.get(url)\n",
    "    time.sleep(2)  # Wait for the page to load\n",
    "    supplier_data = []\n",
    "\n",
    "    suppliers = driver.find_elements(By.CSS_SELECTOR, \"h3 > a\")\n",
    "    for supplier in suppliers:\n",
    "        supplier_name = supplier.text\n",
    "        supplier_url = supplier.get_attribute('href')\n",
    "        supplier_data.append((supplier_name, supplier_url))\n",
    "    return supplier_data\n",
    "\n",
    "all_supplier_data = []\n",
    "\n",
    "for page in range(1, 48):  # ******************CHANGE PAGE NUMBER**************************\n",
    "    all_supplier_data.extend(get_supplier_data(page))\n",
    "    time.sleep(1)  # Wait before loading the next page\n",
    "\n",
    "driver.quit()\n",
    "\n",
    "# Create a DataFrame and save to Excel\n",
    "df = pd.DataFrame(all_supplier_data, columns=['Supplier Name', 'Supplier URL'])\n",
    "df.to_excel('supplier_data_agriculture.xlsx', index=False) #SAVE SUPPLIER_DATA_CATEGORY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090ec1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DOWNLOAD THE SHEET\n",
    "from google.colab import files\n",
    "files.download('suppliers_data_agriculture.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e57bc076",
   "metadata": {},
   "source": [
    "This script is designed to scrape data from a website using Selenium, a tool for automating web browsers. Here's a step-by-step explanation:\n",
    "\n",
    "1. **Importing Libraries:**\n",
    "   - `pandas`: A library for data manipulation and analysis. Used here to create a DataFrame and export it to Excel.\n",
    "   - `selenium`: A tool for automating web browsers. Used to interact with web pages.\n",
    "   - `time`: A standard Python library for handling time-related tasks.\n",
    "\n",
    "2. **Setting Up Chrome Options:**\n",
    "   - `Options()`: Creates an object to set Chrome options.\n",
    "   - `--headless`: Allows Chrome to run without a GUI (Graphical User Interface). This is useful for running the script on servers or automated environments.\n",
    "   - `--no-sandbox`: Disables the Chrome sandboxing feature, which can be necessary in certain environments, particularly on Linux.\n",
    "   - `--disable-dev-shm-usage`: Helps avoid some common issues in containerized environments.\n",
    "\n",
    "3. **Initializing WebDriver:**\n",
    "   - `webdriver.Chrome(options=chrome_options)`: Initializes a new instance of Chrome, applying the options you've set.\n",
    "\n",
    "4. **Defining the `get_supplier_data` Function:**\n",
    "   - This function takes a `page` number as an input.\n",
    "   - Constructs a URL using the given page number and navigates to it with `driver.get(url)`.\n",
    "   - Waits for 2 seconds (`time.sleep(2)`) to ensure the page loads completely.\n",
    "   - Finds all elements matching the CSS selector `\"h3 > a\"` â€“ these are presumably the links to supplier details.\n",
    "   - Extracts the text and 'href' attribute (URL) from each element, storing them in `supplier_data`.\n",
    "\n",
    "5. **Scraping Multiple Pages:**\n",
    "   - An empty list `all_supplier_data` is created to store data from all pages.\n",
    "   - A for loop iterates over a range of page numbers (1 to 47).\n",
    "   - For each page, it calls `get_supplier_data` and extends `all_supplier_data` with the results.\n",
    "   - After each call, it waits for 1 second before proceeding to the next page (`time.sleep(1)`).\n",
    "\n",
    "6. **Closing the WebDriver:**\n",
    "   - `driver.quit()`: Closes the browser window and ends the WebDriver session.\n",
    "\n",
    "7. **Saving Data to Excel:**\n",
    "   - A DataFrame is created from `all_supplier_data` with columns 'Supplier Name' and 'Supplier URL'.\n",
    "   - This DataFrame is then saved to an Excel file named 'supplier_data_agriculture.xlsx'.\n",
    "\n",
    "### Important Notes:\n",
    "\n",
    "- The URL in `get_supplier_data` function is incomplete. You need to insert the actual URL you want to scrape. For example, in case of Agriculture category the link to be used is - \"https://buy.nsw.gov.au/supplier/search?query=&services=agriculture&area=&regions=&schemes=&schemeLevel=&capabilities=&identifiers=&sizes=&page=\" \n",
    "REMEMBER TO EXCLUDE PAGE NUMBER \n",
    "- The page range in the for loop (`range(1, 48)`) should match the actual number of pages you intend to scrape.\n",
    "- The script includes a basic delay (`time.sleep()`) for loading pages and between requests. However, you should be cautious of the website's scraping policy to avoid overloading the server or getting banned.\n",
    "- The script assumes a specific structure of the webpage (suppliers contained in `h3` tags with an `a` tag). If the website's structure is different, you'll need to modify the CSS selector accordingly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
