{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0bd9871",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Efficient Scraping Using Google Colab and Data Slicing\n",
    "\n",
    "Having acquired a list of company names and their corresponding links to the NSW portal in `supplier_data_agriculture.xlsx`, we can now utilize this data to scrape additional details from each company's official profile on the NSW portal. This includes scraping for the company's official website and Australian Business Number (ABN).\n",
    "\n",
    "### Step 1: Setup in Google Colab Notebooks\n",
    "\n",
    "1. **Open New Notebooks in Google Colab:**\n",
    "   - Use separate notebooks to handle different segments of the data for parallel processing.\n",
    "\n",
    "2. **Load the Excel File and Libraries:**\n",
    "   - Import the `supplier_data_agriculture.xlsx` file into each notebook.\n",
    "   - Import necessary libraries such as `pandas` for data manipulation, `selenium` for web scraping, and `re` for regular expressions.\n",
    "\n",
    "### Step 2: Data Slicing for Parallel Processing\n",
    "\n",
    "- **Divide the Dataset:**\n",
    "  - If there are, for instance, 300 rows in `supplier_data_agriculture.xlsx`, divide them for processing in separate notebooks:\n",
    "    - Notebook 1: Rows 1-100\n",
    "    - Notebook 2: Rows 101-200\n",
    "    - Notebook 3: Rows 201-300\n",
    "  - Adjust the slicing in the script accordingly.\n",
    "\n",
    "### Step 3: Scraping Company Websites and ABN\n",
    "\n",
    "1. **Scraping Company Websites:**\n",
    "   - Use the portal links from the sliced data segment in each notebook.\n",
    "   - Implement the `get_supplier_website` function to navigate to each company's page and scrape the URL of the \"Go to supplier website\" hyperlink.\n",
    "   - Store this URL in a new column in your DataFrame.\n",
    "\n",
    "2. **Scraping ABN Details:**\n",
    "   - Continue using the same portal links to scrape the company's ABN.\n",
    "   - Implement the `get_supplier_abn` function, which uses a regular expression to find and extract the 11-digit ABN formatted as 2-3-3-3.\n",
    "   - Add this ABN data to another column in your DataFrame.\n",
    "\n",
    "### Step 4: Executing the Scripts\n",
    "\n",
    "- In each notebook, after adjusting the data slicing, run the scraping scripts:\n",
    "  - The first script scrapes and prints the company's website URL for each supplier.\n",
    "  - The second script scrapes and prints the ABN for each supplier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e097dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "##FOR WEBSITES\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def get_supplier_website(supplier_url):\n",
    "    driver.get(supplier_url)\n",
    "\n",
    "    try:\n",
    "        website_link = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.LINK_TEXT, \"Go to supplier website\"))\n",
    "        )\n",
    "        return website_link.get_attribute('href')\n",
    "    except:\n",
    "        return \"Website link not found\"\n",
    "\n",
    "# read excel file and create a list of supplier URLs. Note this sheet is in Colab cloud so no path is required\n",
    "excel_file_path = 'supplier_data_market.xlsx'  #this is the datasheet containing company name and url (not company website)\n",
    "df = pd.read_excel(excel_file_path)\n",
    "supplier_urls = df['Supplier URL'].tolist()\n",
    "\n",
    "#DO THE SLICING HERE\n",
    "supplier_urls = supplier_urls[:100]\n",
    "\n",
    "# Loop through each supplier URL\n",
    "for url in supplier_urls:\n",
    "    website_url = get_supplier_website(url)\n",
    "    print(url + \" - \" + website_url)\n",
    "\n",
    "# Quit the driver when done\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9af3b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "###FOR ABN\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def get_supplier_abn(supplier_url):\n",
    "    driver.get(supplier_url)\n",
    "\n",
    "    try:\n",
    "        # This is to get all the hyperlinks present in the page\n",
    "        hyperlinks = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_all_elements_located((By.TAG_NAME, \"a\"))\n",
    "        )\n",
    "\n",
    "        # ABN is in the sequence of 2,3,3,3 (total 11 digits)\n",
    "        abn_pattern = re.compile(r'\\d{2} \\d{3} \\d{3} \\d{3}')  # Pattern for ABN\n",
    "\n",
    "        for link in hyperlinks:\n",
    "            if abn_pattern.search(link.text):\n",
    "                return link.text\n",
    "\n",
    "        return \"ABN not found\"\n",
    "    except:\n",
    "        return \"ABN not found\"\n",
    "\n",
    "excel_file_path = 'supplier_data_agriculture.xlsx'\n",
    "df = pd.read_excel(excel_file_path)\n",
    "supplier_urls = df['Supplier URL'].tolist()[:100]  # Slice to the first 100 rows\n",
    "\n",
    "for url in supplier_urls:\n",
    "    abn = get_supplier_abn(url)\n",
    "    print(url, \"-  \", abn)\n",
    "\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eccbf4b",
   "metadata": {},
   "source": [
    "\n",
    "## Managing Scraping Results for Efficiency\n",
    "\n",
    "Once the scraping process is complete, the results will be displayed in the format of `\"NSW Portal Link\" - \"Website/ABN\"`. While saving these results directly to an Excel file is an option, it can be time-consuming, especially if you're dealing with a large amount of data. An efficient alternative is to utilize a two-step copy-paste method involving a text editor (like Notepad or a Notebook).\n",
    "\n",
    "### Step-by-Step Guide to Efficiently Manage Results:\n",
    "\n",
    "1. **Copy the Output:**\n",
    "   - After the scraping script finishes execution, you will have a list of results displayed in your Google Colab notebook.\n",
    "   - Select and copy the entire output from the notebook.\n",
    "\n",
    "2. **Paste into a Text Editor:**\n",
    "   - Open a simple text editor (like Notepad) or use another notebook.\n",
    "   - Paste the copied results into the text editor. \n",
    "   - This step ensures that the data is formatted correctly for the next stage.\n",
    "\n",
    "3. **Copy from the Text Editor:**\n",
    "   - Once pasted into the text editor, select and copy the data again. This step is crucial as it ensures that the data is in a format that will be correctly recognized by Excel. \n",
    "\n",
    "4. **Paste into Excel:**\n",
    "   - Open your existing `supplier_data_agriculture.xlsx` file.\n",
    "   - Paste the data into a new column. \n",
    "   - Since each entry is in the format `\"NSW Portal Link\" - \"Website/ABN\"`, we can neatly align it with the existing rows.\n",
    "\n",
    "5. **Matching Data with Existing Rows:**\n",
    "   - The inclusion of the \"NSW Portal Link\" in the output is intentional and serves an important purpose. \n",
    "   - It allows you to easily match the scraped website URLs or ABNs with the correct rows in your main sheet, based on the NSW Portal Link.\n",
    "\n",
    "### Advantages of This Method:\n",
    "\n",
    "- **Efficiency:** This method is quicker than saving data directly to an Excel file, especially when working with large datasets.\n",
    "- **Accuracy:** By copying from a text editor, you avoid common formatting issues that can arise when pasting data directly from a browser to Excel. Usage of text editor is crucial as it will help recognise each new line as a new row, if we directly copy-paste the entire output in Excel, it will read it in one single cell.\n",
    "- **Ease of Data Matching:** Including the portal link in your output simplifies the process of aligning the new data with the existing rows in your Excel sheet.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
